{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers, losses, callbacks\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "TIME_STEPS = 30  # Number of frames per video\n",
    "SKIP_FRAMES = 20  # Number of frames to skip\n",
    "HEIGHT, WIDTH = 299, 299\n",
    "LSTM_UNITS = 256\n",
    "NUM_CLASSES = 2  # Real and Fake\n",
    "DROPOUT_RATE = 0.5\n",
    "TOTAL_EPOCHS = 25\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON file\n",
    "with open('/teamspace/studios/this_studio/FF++_CElebDF_combined.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extracting training, validation, and test sets\n",
    "train_data = data['training_set']\n",
    "val_data = data['validation_set']\n",
    "test_data = data['testing_set']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store labels and paths\n",
    "def create_labels_dict(dataset):\n",
    "    labels = {}\n",
    "    for entry in dataset:\n",
    "        folder = entry['folder']   # We don't necessarily need 'folder' beyond reference\n",
    "        label = 0 if entry['label'] == 'real' else 1  # Convert label to 0 or 1\n",
    "        path = entry['path']       # Full path to the video folder\n",
    "        labels[folder] = (label, path)  # Store label and path using folder as key\n",
    "    return labels\n",
    "\n",
    "# Create labels and paths for each set\n",
    "train_labels = create_labels_dict(train_data)\n",
    "val_labels = create_labels_dict(val_data)\n",
    "test_labels = create_labels_dict(test_data)\n",
    "\n",
    "# Get the list of folder names (for reference) in each dataset\n",
    "train_IDs = list(train_labels.keys())\n",
    "val_IDs = list(val_labels.keys())\n",
    "test_IDs = list(test_labels.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing function\n",
    "def preprocess_image(image):\n",
    "    image = np.array(image)\n",
    "    image = preprocess_input(image)  # Normalize for the model\n",
    "    return image\n",
    "\n",
    "# Define VideoDataset class to handle paths and labels from JSON\n",
    "class VideoDataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, list_IDs, labels, batch_size, num_frames=30, skip_frames=20, shuffle=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.list_IDs = list_IDs      # Folder names (for referencing)\n",
    "        self.labels = labels          # Dictionary with paths and labels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_frames = num_frames\n",
    "        self.skip_frames = skip_frames\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_IDs = [self.list_IDs[k] for k in indexes]  # Get folder names for batch\n",
    "        X, y = self.__data_generation(batch_IDs)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle indexes after each epoch\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, batch_IDs):\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        for ID in batch_IDs:\n",
    "            label, video_dir = self.labels[ID]  # Get label and path for this folder\n",
    "\n",
    "            # Fetch frames from the directory\n",
    "            frames = sorted(os.listdir(video_dir))\n",
    "\n",
    "            # If folder is empty, skip this video\n",
    "            if len(frames) == 0:\n",
    "                print(f\"No frames found in directory {video_dir}, skipping.\")\n",
    "                continue  # Skip if no frames found\n",
    "\n",
    "            # Apply skip_frames and num_frames logic\n",
    "            frames = frames[self.skip_frames:self.skip_frames + self.num_frames]\n",
    "\n",
    "            # Pad frames if not enough are present\n",
    "            if len(frames) < self.num_frames:\n",
    "                frames += [frames[-1]] * (self.num_frames - len(frames))  # Pad with the last frame\n",
    "\n",
    "            frames_array = []\n",
    "            for frame_name in frames:\n",
    "                frame_path = os.path.join(video_dir, frame_name)  # Use the path to frames\n",
    "                img = Image.open(frame_path).convert('RGB')\n",
    "                img = preprocess_image(img)\n",
    "                frames_array.append(img)\n",
    "\n",
    "            X.append(frames_array)\n",
    "            y.append(label)\n",
    "\n",
    "        X = np.array(X)  # Shape: (batch_size, num_frames, H, W, C)\n",
    "        y = np.array(y)\n",
    "\n",
    "        # Convert labels to one-hot encoding\n",
    "        # y = tf.keras.utils.to_categorical(y, num_classes=2)\n",
    "        \n",
    "        # Optional: check shapes\n",
    "        # print(\"X shape:\", X.shape)\n",
    "        # print(\"y shape:\", y.shape)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = VideoDataset(train_IDs, train_labels, BATCH_SIZE, num_frames=TIME_STEPS, skip_frames=SKIP_FRAMES, shuffle=True)\n",
    "val_dataset = VideoDataset(val_IDs, val_labels, BATCH_SIZE, num_frames=TIME_STEPS, skip_frames=SKIP_FRAMES, shuffle=False)\n",
    "test_dataset = VideoDataset(test_IDs, test_labels, BATCH_SIZE, num_frames=TIME_STEPS, skip_frames=SKIP_FRAMES, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "def build_model(lstm_hidden_size=256, num_classes=2, dropout_rate=0.5):\n",
    "    inputs = layers.Input(shape=(TIME_STEPS, HEIGHT, WIDTH, 3))\n",
    "\n",
    "    base_model = keras.applications.Xception(weights='imagenet', include_top=False, pooling='avg')\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False  # Freeze base model layers initially\n",
    "\n",
    "    x = layers.TimeDistributed(base_model)(inputs)\n",
    "    x = layers.LSTM(lstm_hidden_size)(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "model = build_model(lstm_hidden_size=LSTM_UNITS, num_classes=NUM_CLASSES, dropout_rate=DROPOUT_RATE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.saving import register_keras_serializable\n",
    "\n",
    "@register_keras_serializable()\n",
    "def recall_m(y_true, y_pred):\n",
    "    # Convert predictions to binary class labels (0 or 1)\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)  # Get predicted class from probabilities\n",
    "    \n",
    "    # True positives (predicted 1 and true 1)\n",
    "    true_positives = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 1), tf.equal(y_pred, 1)), tf.float32))\n",
    "    \n",
    "    # Actual positives (true 1)\n",
    "    actual_positives = tf.reduce_sum(tf.cast(tf.equal(y_true, 1), tf.float32))\n",
    "    \n",
    "    # Recall calculation with epsilon for numerical stability\n",
    "    recall = true_positives / (actual_positives + tf.keras.backend.epsilon())\n",
    "    return recall\n",
    "\n",
    "@register_keras_serializable()\n",
    "def precision_m(y_true, y_pred):\n",
    "    # Convert predictions to binary class labels (0 or 1)\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)  # Get predicted class from probabilities\n",
    "    \n",
    "    # True positives (predicted 1 and true 1)\n",
    "    true_positives = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 1), tf.equal(y_pred, 1)), tf.float32))\n",
    "    \n",
    "    # Predicted positives (predicted 1)\n",
    "    predicted_positives = tf.reduce_sum(tf.cast(tf.equal(y_pred, 1), tf.float32))\n",
    "    \n",
    "    # Precision calculation with epsilon for numerical stability\n",
    "    precision = true_positives / (predicted_positives + tf.keras.backend.epsilon())\n",
    "    return precision\n",
    "\n",
    "@register_keras_serializable()\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))\n",
    "    return f1\n",
    "\n",
    "# Now you can load the model in one of two ways:\n",
    "\n",
    "# Option 1: Using the registered metrics (preferred)\n",
    "model = keras.models.load_model('/teamspace/studios/this_studio/MODELS/Trial_4.69 Phase2.keras')\n",
    "\n",
    "# Option 2: Passing custom objects explicitly\n",
    "custom_objects = {\n",
    "    'f1_m': f1_m,\n",
    "    'precision_m': precision_m,\n",
    "    'recall_m': recall_m\n",
    "}\n",
    "model = keras.models.load_model('/teamspace/studios/this_studio/XL_best_models/BEST_CELEB_DF_Phase2.keras', \n",
    "                              custom_objects=custom_objects)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Phase 1\n",
    "for layer in model.layers[1].layer.layers[:]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# # Re-compile the model with a lower learning rate\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "              loss=losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy', f1_m,precision_m,recall_m])\n",
    "model_save_dir='/teamspace/studios/this_studio/XL_best_models'\n",
    "# Callbacks for Phase 1\n",
    "checkpoint_phase1 = callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(model_save_dir, 'COMBINED_best_Phase1.keras'),\n",
    "    monitor='val_f1_m',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    verbose=1,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "early_stopping_phase1 = callbacks.EarlyStopping(\n",
    "    monitor='val_f1_m',\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "num_epochs_phase2 =50\n",
    "\n",
    "# Update train_dataset and val_dataset with the new batch size\n",
    "train_dataset.batch_size = BATCH_SIZE\n",
    "val_dataset.batch_size = BATCH_SIZE\n",
    "\n",
    "\n",
    "history_phase1 = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=num_epochs_phase2,\n",
    "    callbacks=[checkpoint_phase1, early_stopping_phase1],\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Phase 2\n",
    "for layer in model.layers[1].layer.layers[:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# # Re-compile the model with a lower learning rate\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=1e-5),\n",
    "              loss=losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy', f1_m,precision_m,recall_m])\n",
    "model_save_dir='/teamspace/studios/this_studio/XL_best_models'\n",
    "# Callbacks for Phase 2\n",
    "checkpoint_phase2 = callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(model_save_dir, 'COMBINED_best_Phase2.keras'),\n",
    "    monitor='val_f1_m',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    verbose=1,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "early_stopping_phase2 = callbacks.EarlyStopping(\n",
    "    monitor='val_f1_m',\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "num_epochs_phase2 = 50\n",
    "\n",
    "# Update train_dataset and val_dataset with the new batch size\n",
    "train_dataset.batch_size = BATCH_SIZE\n",
    "val_dataset.batch_size = BATCH_SIZE\n",
    "\n",
    "\n",
    "history_phase2 = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=num_epochs_phase2,\n",
    "    callbacks=[checkpoint_phase2, early_stopping_phase2],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Load the best model from Phase 2\n",
    "model.load_weights(os.path.join(model_save_dir, 'CELEB_DF_Phase2.keras'))\n",
    "\n",
    "# Evaluate on test set\n",
    "# Update test_dataset with the new batch size\n",
    "test_dataset.batch_size = BATCH_SIZE\n",
    "test_loss, test_acc = model.evaluate(test_dataset, verbose=1)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
